---
layout: post
title: Blog Post 3 - Fake News Classifier
---


```python
import pandas as pd
import tensorflow as tf
import numpy as np
```


```python
from matplotlib import pyplot as plt

```


```python
#train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv('fake_news_train.csv')
```


```python
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>10709</td>
      <td>ALARMING: NSA Refuses to Release Clinton-Lynch...</td>
      <td>If Clinton and Lynch just talked about grandki...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>8731</td>
      <td>Can Pence's vow not to sling mud survive a Tru...</td>
      <td>() - In 1990, during a close and bitter congre...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>4733</td>
      <td>Watch Trump Campaign Try To Spin Their Way Ou...</td>
      <td>A new ad by the Hillary Clinton SuperPac Prior...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>3993</td>
      <td>Trump celebrates first 100 days as president, ...</td>
      <td>HARRISBURG, Pa.U.S. President Donald Trump hit...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>12896</td>
      <td>TRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...</td>
      <td>MELBOURNE, FL is a town with a population of 7...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 4 columns</p>
</div>




```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_df = pd.read_csv(test_url)
```

## Make Dataset

- Remove stopwords from the article text and title. A stopword is a word that is usually considered to be uninformative, such as “the,” “and,” or “but.” You may find this StackOverFlow thread to be helpful.
- Construct and return a tf.data.Dataset with two inputs and one output. The input should be of the form (title, text), and the output should consist only of the fake column. You may find it helpful to consult these lecture notes or this tutorial for reference on how to construct and use Datasets with multiple inputs.

### Remove stopwords and create tf dataset


```python
import nltk
#nltk.download('stopwords')
#nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize 
stop = stopwords.words('english')
```




```python
from nltk.tokenize.treebank import TreebankWordDetokenizer
```


```python
def make_dataset(df):
    df_ = df
    #remove stopwords for the dataframe
    df_['title'] = df_['title'].apply(word_tokenize)
    df_['title'] = df['title'].apply(lambda x: [w for w in x if not w in stop] )
    df_['text'] = df_['text'].apply(word_tokenize)
    df_['text'] = df_['text'].apply(lambda x: [w for w in x if not w in stop] )
    df_['text'] = df_['text'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))
    df_['title'] = df_['title'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))

    #create tensorflow dataset
    return tf.data.Dataset.from_tensor_slices(({"title": df_['title'], "text": df_['text']}, df_['fake']))
```


```python
dataset = make_dataset(df)
```


```python
test_dataset = make_dataset(test_df)
```

### Train Test Split, Batch


```python
dataset = dataset.shuffle(buffer_size = len(dataset))

train_size = int(0.7*len(dataset))
val_size   = int(0.2*len(dataset))

train = dataset.take(train_size).batch(100)
val   = dataset.skip(train_size).take(val_size).batch(100)
#test  = dataset.skip(train_size + val_size).batch(100) #last 0.1 of data

len(train), len(val)#, len(test)
```




    (158, 45)




```python
test_ds = test_dataset.batch(100)
```

## Create Models

### Vectorization layer


```python
def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 
```


```python
from tensorflow.keras import losses
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

import re
import string
```


```python
# only the top distinct words will be tracked
max_tokens = 2000

# each headline will be a vector of length 20
sequence_length = 20

vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=max_tokens, # only consider this many words
    output_mode='int',
    output_sequence_length=sequence_length) 
```


```python
vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
```

When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?




```python
from tensorflow import keras
from tensorflow.keras import layers
```


```python
title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```

### Model trains with titles only


```python
x = vectorize_layer(title_input)
x = layers.Embedding(max_tokens, output_dim = 5, name = "embedding_tl")(x)
x = layers.Dropout(0.2)(x)
x = layers.GlobalAveragePooling1D()(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(32, activation='relu')(x)
output = layers.Dense(2, name = "fake")(x)
```


```python
model_tl = keras.Model(
    inputs = title_input,
    outputs = output
)
```


```python
model_tl.summary()
```

    Model: "model"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    title (InputLayer)           [(None, 1)]               0         
    _________________________________________________________________
    text_vectorization (TextVect (None, 20)                0         
    _________________________________________________________________
    embedding_tl (Embedding)     (None, 20, 5)             10000     
    _________________________________________________________________
    dropout (Dropout)            (None, 20, 5)             0         
    _________________________________________________________________
    global_average_pooling1d (Gl (None, 5)                 0         
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 5)                 0         
    _________________________________________________________________
    dense (Dense)                (None, 32)                192       
    _________________________________________________________________
    fake (Dense)                 (None, 2)                 66        
    =================================================================
    Total params: 10,258
    Trainable params: 10,258
    Non-trainable params: 0
    _________________________________________________________________



```python
model_tl.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = model_tl.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = False)
```




```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




 



    
![blogpost3_31_1.png](/images/blogpost3_files/blogpost3_31_1.png)
    



```python
model_tl.evaluate(test_ds)
```

    225/225 [==============================] - 0s 1ms/step - loss: 0.0840 - accuracy: 0.9752





    [0.08402662724256516, 0.9751881957054138]



### Model trains with text only


```python
x_ = vectorize_layer(text_input)
x_ = layers.Embedding(max_tokens, output_dim = 5, name = "embedding_tx")(x_)
x_ = layers.Dropout(0.2)(x_)
x_ = layers.GlobalAveragePooling1D()(x_)
x_ = layers.Dropout(0.2)(x_)
x_ = layers.Dense(32, activation='relu')(x_)
output = layers.Dense(2, name = "fake")(x_)
```


```python
model_tx= keras.Model(
    inputs = text_input,
    outputs = output
)
```


```python
model_tx.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
model_tx.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = False)
```

    /Users/feishu/opt/anaconda3/envs/PIC16B/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      [n for n in tensors.keys() if n not in ref_input_names])





    <tensorflow.python.keras.callbacks.History at 0x7f84d88366d0>




```python
model_tx.evaluate(test_ds)
```

    225/225 [==============================] - 1s 6ms/step - loss: 0.1612 - accuracy: 0.9463





    [0.16124901175498962, 0.9462782144546509]



### Model trained with both text and title


```python
both = layers.concatenate([x, x_], axis = 1)
output_both = layers.Dense(2, name = "fake")(both)
```


```python
model_both= keras.Model(
    inputs = [title_input, text_input],
    outputs = output_both
)
```


```python
model_both.summary()
```

    Model: "model_2"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    title (InputLayer)              [(None, 1)]          0                                            
    __________________________________________________________________________________________________
    text (InputLayer)               [(None, 1)]          0                                            
    __________________________________________________________________________________________________
    text_vectorization (TextVectori (None, 20)           0           title[0][0]                      
                                                                     text[0][0]                       
    __________________________________________________________________________________________________
    embedding_tl (Embedding)        (None, 20, 5)        10000       text_vectorization[0][0]         
    __________________________________________________________________________________________________
    embedding_tx (Embedding)        (None, 20, 5)        10000       text_vectorization[1][0]         
    __________________________________________________________________________________________________
    dropout (Dropout)               (None, 20, 5)        0           embedding_tl[0][0]               
    __________________________________________________________________________________________________
    dropout_2 (Dropout)             (None, 20, 5)        0           embedding_tx[0][0]               
    __________________________________________________________________________________________________
    global_average_pooling1d (Globa (None, 5)            0           dropout[0][0]                    
    __________________________________________________________________________________________________
    global_average_pooling1d_1 (Glo (None, 5)            0           dropout_2[0][0]                  
    __________________________________________________________________________________________________
    dropout_1 (Dropout)             (None, 5)            0           global_average_pooling1d[0][0]   
    __________________________________________________________________________________________________
    dropout_3 (Dropout)             (None, 5)            0           global_average_pooling1d_1[0][0] 
    __________________________________________________________________________________________________
    dense (Dense)                   (None, 32)           192         dropout_1[0][0]                  
    __________________________________________________________________________________________________
    dense_1 (Dense)                 (None, 32)           192         dropout_3[0][0]                  
    __________________________________________________________________________________________________
    concatenate (Concatenate)       (None, 64)           0           dense[0][0]                      
                                                                     dense_1[0][0]                    
    __________________________________________________________________________________________________
    fake (Dense)                    (None, 2)            130         concatenate[0][0]                
    ==================================================================================================
    Total params: 20,514
    Trainable params: 20,514
    Non-trainable params: 0
    __________________________________________________________________________________________________



```python
model_both.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
model_both.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = False)
```




    <tensorflow.python.keras.callbacks.History at 0x7f84c388e910>




```python
model_both.evaluate(test_ds)
```

    225/225 [==============================] - 1s 6ms/step - loss: 0.0371 - accuracy: 0.9913





    [0.037129346281290054, 0.991313636302948]



## Embedding Analysis


```python
weights_tl = model_both.get_layer('embedding_tl').get_weights()[0]
weights_tx = model_both.get_layer('embedding_tx').get_weights()[0]# get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()  
```


```python
vocab = vectorize_layer.get_vocabulary()  
```


```python
weights_tl
```




    array([[ 0.10268341,  0.09916885, -0.11284392,  0.12111397, -0.10344727],
           [-0.10890676, -0.13384679,  0.10487777, -0.12676527,  0.13442   ],
           [ 0.05327572,  0.05760193, -0.11840718, -0.00806848, -0.11700375],
           ...,
           [ 0.02861674,  0.02574723,  0.04686182,  0.09227268, -0.03490727],
           [ 0.15639482,  0.09743544, -0.06591856,  0.02370312, -0.01798774],
           [ 0.00066107, -0.01369754, -0.06120993,  0.03191474, -0.09007049]],
          dtype=float32)




```python
from sklearn.decomposition import PCA
```


```python
pca = PCA(n_components=2)
weights_tl = pca.fit_transform(weights_tl)
weights_tx = pca.fit_transform(weights_tx)
```


```python
len(vocab)
```




    2000




```python
embedding_tl = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights_tl[:,0],
    'x1'   : weights_tl[:,1]
})
embedding_tl
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td></td>
      <td>-0.195497</td>
      <td>0.003443</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[UNK]</td>
      <td>0.317627</td>
      <td>0.025609</td>
    </tr>
    <tr>
      <th>2</th>
      <td>said</td>
      <td>-0.105505</td>
      <td>-0.010339</td>
    </tr>
    <tr>
      <th>3</th>
      <td>trump</td>
      <td>0.100054</td>
      <td>-0.045148</td>
    </tr>
    <tr>
      <th>4</th>
      <td>the</td>
      <td>1.743519</td>
      <td>-0.003649</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1995</th>
      <td>tonight</td>
      <td>0.910266</td>
      <td>0.059968</td>
    </tr>
    <tr>
      <th>1996</th>
      <td>repeated</td>
      <td>0.170522</td>
      <td>0.024102</td>
    </tr>
    <tr>
      <th>1997</th>
      <td>projects</td>
      <td>-0.014011</td>
      <td>-0.052086</td>
    </tr>
    <tr>
      <th>1998</th>
      <td>outcome</td>
      <td>-0.117215</td>
      <td>0.046006</td>
    </tr>
    <tr>
      <th>1999</th>
      <td>launch</td>
      <td>-0.029681</td>
      <td>-0.028275</td>
    </tr>
  </tbody>
</table>
<p>2000 rows × 3 columns</p>
</div>




```python
embedding_tx = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights_tx[:,0],
    'x1'   : weights_tx[:,1]
})
```


```python
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
import plotly.express as px 
```


```python
fig = px.scatter( embedding_tl, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_tl))),
                 size_max = 2,
                 hover_name = "word")

fig.show()
```


{% include emb-tx.html %}


```python

```
