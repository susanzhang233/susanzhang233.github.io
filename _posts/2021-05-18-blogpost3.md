---
layout: post
title: Blog Post 3 - Fake News Classifier
---

In this blog, we'll build a machine learning model with Tensorflow that helps us to classify fake news.

First, we'll import some of the needed packages our dataset to feed the model.


```python
import pandas as pd
import tensorflow as tf
import numpy as np
```


```python
from matplotlib import pyplot as plt
```

Both the training and testing datasets are accessed from [Kaggle](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset):


```python
#train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv('fake_news_train.csv')
```


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_df = pd.read_csv(test_url)
```


```python
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>10709</td>
      <td>ALARMING: NSA Refuses to Release Clinton-Lynch...</td>
      <td>If Clinton and Lynch just talked about grandki...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>8731</td>
      <td>Can Pence's vow not to sling mud survive a Tru...</td>
      <td>() - In 1990, during a close and bitter congre...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>4733</td>
      <td>Watch Trump Campaign Try To Spin Their Way Ou...</td>
      <td>A new ad by the Hillary Clinton SuperPac Prior...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>3993</td>
      <td>Trump celebrates first 100 days as president, ...</td>
      <td>HARRISBURG, Pa.U.S. President Donald Trump hit...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>12896</td>
      <td>TRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...</td>
      <td>MELBOURNE, FL is a town with a population of 7...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 4 columns</p>
</div>



Apart from the indices column, the dataframe contains three more informative columns: title column, the titles of the news; text column, the content of that news; the fake column, label indicating whether the news is 0(real), or 1(fake).

## Make Dataset

Before we actually start the training process, transforming the dataset into decent forms for training are also very important. Here, we'll create a `make_dataset` function that'll do two things:

- remove stop words from the article text and title columns(*stop words* refers to those most common words in a language, which could be filtered out for training. Some common stop words are a, the, as, at, by, to, etc)

- construct a tensorflow dataset with inputs (title, text) and output fake column labels. Dataset format of enables the input to be smoothly handled by tensorflow  (tensorflow [*dataset*](https://www.tensorflow.org/guide/data) is a specific class containing many useful functions to keep you organized)

To remove stop words, we'll first import some extra packages and functions.
- the `stop words` function gives you a list of common stop words of specified language;
- the  `word_tokenize` function breaks a string into single words and punctuations of a 1d array form;
- the `TreebankWordDetokenizer` function detokenizes a 1d array of words, combining it back into one individual string.


```python
import nltk
#nltk.download('stopwords') #only need to be downloaded once
#nltk.download('punkt')  #only need to be downloaded once as well
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize 
stop = stopwords.words('english')
```





```python
from nltk.tokenize.treebank import TreebankWordDetokenizer
```


```python
def make_dataset(df):
    df_ = df
    
    #remove stopwords for the dataframe
    df_['title'] = df_['title'].apply(word_tokenize)# split a string into separate tokens
    df_['title'] = df['title'].apply(lambda x: [w for w in x if not w in stop] )#remove stopwords
    df_['text'] = df_['text'].apply(word_tokenize)
    df_['text'] = df_['text'].apply(lambda x: [w for w in x if not w in stop] )
    df_['text'] = df_['text'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))#combine the tokens back to a complete sentence
    df_['title'] = df_['title'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))

    #create tensorflow dataset with inputs (title, text) and output fake column labels and return it
    return tf.data.Dataset.from_tensor_slices(({"title": df_['title'], "text": df_['text']}, df_['fake']))
```

We then apply the function to our training and testing datasets:


```python
dataset = make_dataset(df)
```


```python
test_dataset = make_dataset(test_df)
```

### Train Validation Split, Create Batches

Now we are ready to construct some train & validation split inside our training set. Along with that, we'll also set batches for training set, as training with the original dataset volume might be too inefficient for demonstration purposes.


```python
#shuffle the complete training set
dataset = dataset.shuffle(buffer_size = len(dataset))

#specify training and validation size: 0.7 & 0.2, respectively
train_size = int(0.7*len(dataset))
val_size   = int(0.2*len(dataset))

#pick out first 0.7 as training set, with a batch of 100
train = dataset.take(train_size).batch(100)
#pick out the next 0.2 as validation set, with a batch of 100
val   = dataset.skip(train_size).take(val_size).batch(100)
#test  = dataset.skip(train_size + val_size).batch(100) #last 0.1 of data

#check for the length of the two sets
len(train), len(val)#, len(test)
```




    (158, 45)




```python
test_ds = test_dataset.batch(100)
```

## Create Models

We are now ready to make our models. While our data contains informative columns of both news title and news text, we would like to know whether titles only, text only, or both, would be more effective for detection of fake news. Therefore, we'll be creating three models, with similar strategies, and comparing their accuracies.

### Vectorization layer

No matter which combination we'll be using, we all need to first *vectorize* the input strings. Vectorization refers to processing of the strings into computer readable format, which is, numbers. There are multiple ways of vectorization of strings, such as the [term-document matrix](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16A/blob/master/content/NLP/NLP_1.ipynb), etc. Here, we'll be using the `TextVectorization` function provided by tensorflow. This function would represent each of the words by its rank of frequency in the whole dataset. First, we'll need to define a `standardization` function to supply to the `TextVectorization` function.

Here, the `standardization` function would convert all words to lowercases and remove all punctuations.

```python
def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 
```

Then, with the `TextVectorization` function, we'll make a *vectorize_layer* that transforms each words in the training set, after standardization treatment, into tensors with the words' corresponding *frequency rank*.

```python
from tensorflow.keras import losses
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

import re
import string
```


```python
# only the first 2000 distinct words in the whole set will be tracked
max_tokens = 2000

# only the first 20 words of each headline will be considered
sequence_length = 20

vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=max_tokens, # only consider this many words
    output_mode='int',
    output_sequence_length=sequence_length) 
```
After creating the layers, we would still need to adapt the layer to our set. By adapting it, we will ensure that the vectorization layer have the words' corresponding frequency ranks fixed. Thus whenever we are training or testing with that layer, each distinct word would have that one and only ranking.

```python
vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
```




Then, we'll specify two input layers for title and text.

```python
from tensorflow import keras
from tensorflow.keras import layers
```


```python
title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```

### Model trains with titles only
Since we have two kinds of inputs, title and text, we'll be using the keras *functional* API, instead of *sequential* API. The *functional* API can create layers that have more flexible sequences, therefore suitable for building a model with multiple inputs. First, we'll build a model that learns the news titles only.

Our first layer would be the vectorize_layer we just built. In each of the following layers, we supply the input as the result obtained from the preceding layer in *()*. 
With [`Dropout`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout), the input units in that layer will be set to 0 with a frequency of *rate* at each step during training time, which helps prevent overfitting.
The [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer creates a vocabulary space, giving each word its own unique spot in that space. Furthermore, the spots of the words are set in a way that the distance and direction between words would indicate relatedness and relationships. We'll give this layer a name "embedding_tl" to ease the process of examining it later.

```python
x = vectorize_layer(title_input)
x = layers.Embedding(max_tokens, output_dim = 5, name = "embedding_tl")(x)
x = layers.Dropout(0.2)(x)
x = layers.GlobalAveragePooling1D()(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(32, activation='relu')(x)
output = layers.Dense(2, name = "fake")(x)
```

Now, we are finally ready to build the model. Only specifying the initial input, tensorflow will be able to extract all that information we've provided of the layers.
```python
model_tl = keras.Model(
    inputs = title_input,
    outputs = output
)
```


```python
model_tl.summary()
```

    Model: "model"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    title (InputLayer)           [(None, 1)]               0         
    _________________________________________________________________
    text_vectorization (TextVect (None, 20)                0         
    _________________________________________________________________
    embedding_tl (Embedding)     (None, 20, 5)             10000     
    _________________________________________________________________
    dropout (Dropout)            (None, 20, 5)             0         
    _________________________________________________________________
    global_average_pooling1d (Gl (None, 5)                 0         
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 5)                 0         
    _________________________________________________________________
    dense (Dense)                (None, 32)                192       
    _________________________________________________________________
    fake (Dense)                 (None, 2)                 66        
    =================================================================
    Total params: 10,258
    Trainable params: 10,258
    Non-trainable params: 0
    _________________________________________________________________

As always, we'll compile the model and fit it to the training set.

```python
model_tl.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = model_tl.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = False)
```

And we'll plot out the accuracy curve.

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```








    
![blogpost3_31_1.png](/images/blogpost3_files/blogpost3_31_1.png)
    



```python
model_tl.evaluate(test_ds)
```

    225/225 [==============================] - 0s 1ms/step - loss: 0.0840 - accuracy: 0.9752





    [0.08402662724256516, 0.9751881957054138]


Evaluating the model with only titles with out test set, we could see that the accuracy is actually pretty high: 0.97! Let's see if the other two models could do better.S

### Model trains with text only

Training with text would have a similar process: specify each layers, make the model, compile it, fit it, evaluate and observe.

```python
x_ = vectorize_layer(text_input)
x_ = layers.Embedding(max_tokens, output_dim = 5, name = "embedding_tx")(x_)
x_ = layers.Dropout(0.2)(x_)
x_ = layers.GlobalAveragePooling1D()(x_)
x_ = layers.Dropout(0.2)(x_)
x_ = layers.Dense(32, activation='relu')(x_)
output = layers.Dense(2, name = "fake")(x_)
```


```python
model_tx= keras.Model(
    inputs = text_input,
    outputs = output
)
```


```python
model_tx.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
model_tx.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = False)
```

 







```python
model_tx.evaluate(test_ds)
```

    225/225 [==============================] - 1s 6ms/step - loss: 0.1612 - accuracy: 0.9463





    [0.16124901175498962, 0.9462782144546509]

The accuracy for the model trained with the text only has a lower accuracy. Somehow reasonable, as the text is longer, and a fake news might also possess some sentences that *sounds* true.

### Model trained with both text and title


```python
both = layers.concatenate([x, x_], axis = 1)
output_both = layers.Dense(2, name = "fake")(both)
```


```python
model_both= keras.Model(
    inputs = [title_input, text_input],
    outputs = output_both
)
```


```python
model_both.summary()
```

    Model: "model_2"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    title (InputLayer)              [(None, 1)]          0                                            
    __________________________________________________________________________________________________
    text (InputLayer)               [(None, 1)]          0                                            
    __________________________________________________________________________________________________
    text_vectorization (TextVectori (None, 20)           0           title[0][0]                      
                                                                     text[0][0]                       
    __________________________________________________________________________________________________
    embedding_tl (Embedding)        (None, 20, 5)        10000       text_vectorization[0][0]         
    __________________________________________________________________________________________________
    embedding_tx (Embedding)        (None, 20, 5)        10000       text_vectorization[1][0]         
    __________________________________________________________________________________________________
    dropout (Dropout)               (None, 20, 5)        0           embedding_tl[0][0]               
    __________________________________________________________________________________________________
    dropout_2 (Dropout)             (None, 20, 5)        0           embedding_tx[0][0]               
    __________________________________________________________________________________________________
    global_average_pooling1d (Globa (None, 5)            0           dropout[0][0]                    
    __________________________________________________________________________________________________
    global_average_pooling1d_1 (Glo (None, 5)            0           dropout_2[0][0]                  
    __________________________________________________________________________________________________
    dropout_1 (Dropout)             (None, 5)            0           global_average_pooling1d[0][0]   
    __________________________________________________________________________________________________
    dropout_3 (Dropout)             (None, 5)            0           global_average_pooling1d_1[0][0] 
    __________________________________________________________________________________________________
    dense (Dense)                   (None, 32)           192         dropout_1[0][0]                  
    __________________________________________________________________________________________________
    dense_1 (Dense)                 (None, 32)           192         dropout_3[0][0]                  
    __________________________________________________________________________________________________
    concatenate (Concatenate)       (None, 64)           0           dense[0][0]                      
                                                                     dense_1[0][0]                    
    __________________________________________________________________________________________________
    fake (Dense)                    (None, 2)            130         concatenate[0][0]                
    ==================================================================================================
    Total params: 20,514
    Trainable params: 20,514
    Non-trainable params: 0
    __________________________________________________________________________________________________



```python
model_both.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
model_both.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = False)
```




    <tensorflow.python.keras.callbacks.History at 0x7f84c388e910>




```python
model_both.evaluate(test_ds)
```

    225/225 [==============================] - 1s 6ms/step - loss: 0.0371 - accuracy: 0.9913





    [0.037129346281290054, 0.991313636302948]

Here, we could observe that training with both text and title yields an accuracy of 0.9913!

## Embedding Analysis
In the next stage, we'll extract the previous *Embedding* layer, do some visualization of it to demonstrate how the words are learned by the model. We'll plot out each word's spot in the whole 2d vocabulary space. Since our embedding layer has an output of 5 dimensions, we'll first need to conduct principle component analysis to transform it into 2d representations. Sci-kit learn has that function created for us.

First, we'll get the vocabularies and weights corresponding to the words in the two embedding layers of title/text out:
```python
weights_tl = model_both.get_layer('embedding_tl').get_weights()[0]
weights_tx = model_both.get_layer('embedding_tx').get_weights()[0]# get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()  
```


```python
vocab = vectorize_layer.get_vocabulary()  
```


```python
weights_tl
```




    array([[ 0.10268341,  0.09916885, -0.11284392,  0.12111397, -0.10344727],
           [-0.10890676, -0.13384679,  0.10487777, -0.12676527,  0.13442   ],
           [ 0.05327572,  0.05760193, -0.11840718, -0.00806848, -0.11700375],
           ...,
           [ 0.02861674,  0.02574723,  0.04686182,  0.09227268, -0.03490727],
           [ 0.15639482,  0.09743544, -0.06591856,  0.02370312, -0.01798774],
           [ 0.00066107, -0.01369754, -0.06120993,  0.03191474, -0.09007049]],
          dtype=float32)




Next, we perform PCA to the weights to convert them into 2d arrays:
```python
from sklearn.decomposition import PCA
```


```python
pca = PCA(n_components=2)
weights_tl = pca.fit_transform(weights_tl)
weights_tx = pca.fit_transform(weights_tx)
```


```python
len(vocab)
```




    2000



Then, we'll create a data frame containing the embedding of words in titles.
```python
embedding_tl = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights_tl[:,0],
    'x1'   : weights_tl[:,1]
})
embedding_tl
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td></td>
      <td>-0.195497</td>
      <td>0.003443</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[UNK]</td>
      <td>0.317627</td>
      <td>0.025609</td>
    </tr>
    <tr>
      <th>2</th>
      <td>said</td>
      <td>-0.105505</td>
      <td>-0.010339</td>
    </tr>
    <tr>
      <th>3</th>
      <td>trump</td>
      <td>0.100054</td>
      <td>-0.045148</td>
    </tr>
    <tr>
      <th>4</th>
      <td>the</td>
      <td>1.743519</td>
      <td>-0.003649</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1995</th>
      <td>tonight</td>
      <td>0.910266</td>
      <td>0.059968</td>
    </tr>
    <tr>
      <th>1996</th>
      <td>repeated</td>
      <td>0.170522</td>
      <td>0.024102</td>
    </tr>
    <tr>
      <th>1997</th>
      <td>projects</td>
      <td>-0.014011</td>
      <td>-0.052086</td>
    </tr>
    <tr>
      <th>1998</th>
      <td>outcome</td>
      <td>-0.117215</td>
      <td>0.046006</td>
    </tr>
    <tr>
      <th>1999</th>
      <td>launch</td>
      <td>-0.029681</td>
      <td>-0.028275</td>
    </tr>
  </tbody>
</table>
<p>2000 rows × 3 columns</p>
</div>


We'll do the same thing with the embedding of words in the news text.

```python
embedding_tx = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights_tx[:,0],
    'x1'   : weights_tx[:,1]
})
```

Now, we are finally ready to plot the words and their embeddings out！We'll be using plotly express function!
```python
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
import plotly.express as px 
```


```python
fig = px.scatter( embedding_tl, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_tl))),
                 size_max = 2,
                 hover_name = "word")

fig.show()
```
{% include emb-tx.html %}

Here, the graph demonstrates each words' spot in the vocabulary space. 

```python
from plotly.io import write_html
```


```python
write_html(fig, "emb_tx.html")
```


```python

```
