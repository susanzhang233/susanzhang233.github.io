---
layout: post
title: Blog Post 3 - Fake News Classifier
---

In this blog, we'll build a machine learning model with Tensorflow that helps us to classify fake news.

First, we'll import some of the needed packages our dataset to feed the model.


```python
import pandas as pd
import tensorflow as tf
import numpy as np
```


```python
from matplotlib import pyplot as plt
```

Both the training and testing datasets are accessed from [Kaggle](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset):


```python
#train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv('fake_news_train.csv')
```


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_df = pd.read_csv(test_url)
```


```python
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>10709</td>
      <td>ALARMING: NSA Refuses to Release Clinton-Lynch...</td>
      <td>If Clinton and Lynch just talked about grandki...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>8731</td>
      <td>Can Pence's vow not to sling mud survive a Tru...</td>
      <td>() - In 1990, during a close and bitter congre...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>4733</td>
      <td>Watch Trump Campaign Try To Spin Their Way Ou...</td>
      <td>A new ad by the Hillary Clinton SuperPac Prior...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>3993</td>
      <td>Trump celebrates first 100 days as president, ...</td>
      <td>HARRISBURG, Pa.U.S. President Donald Trump hit...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>12896</td>
      <td>TRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...</td>
      <td>MELBOURNE, FL is a town with a population of 7...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 4 columns</p>
</div>



Apart from the indices column, the dataframe contains three more informative columns: title column, the titles of the news; text column, the content of that news; the fake column, label indicating whether the news is 0(real), or 1(fake).

## Make Dataset

Before we actually start the training process, transforming the dataset into decent forms for training are also very important. Here, we'll create a `make_dataset` function that'll do two things:

- remove stop words from the article text and title columns(*stop words* refers to those most common words in a language, which could be filtered out for training. Some common stop words are a, the, as, at, by, to, etc)

- construct a tensorflow dataset with inputs (title, text) and output fake column labels. Dataset format of enables the input to be smoothly handled by tensorflow  (tensorflow [*dataset*](https://www.tensorflow.org/guide/data) is a specific class containing many useful functions to keep you organized)

To remove stop words, we'll first import some extra packages and functions.
- the `stop words` function gives you a list of common stop words of specified language;
- the  `word_tokenize` function breaks a string into single words and punctuations of a 1d array form;
- the `TreebankWordDetokenizer` function detokenizes a 1d array of words, combining it back into one individual string.


```python
import nltk
#nltk.download('stopwords') #only need to be downloaded once
#nltk.download('punkt')  #only need to be downloaded once as well
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize 
stop = stopwords.words('english')
```





```python
from nltk.tokenize.treebank import TreebankWordDetokenizer
```


```python
def make_dataset(df):
    df_ = df
    
    #remove stopwords for the dataframe
    df_['title'] = df_['title'].apply(word_tokenize)# split a string into separate tokens
    df_['title'] = df['title'].apply(lambda x: [w for w in x if not w in stop] )#remove stopwords
    df_['text'] = df_['text'].apply(word_tokenize)
    df_['text'] = df_['text'].apply(lambda x: [w for w in x if not w in stop] )
    df_['text'] = df_['text'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))#combine the tokens back to a complete sentence
    df_['title'] = df_['title'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))

    #create tensorflow dataset with inputs (title, text) and output fake column labels and return it
    return tf.data.Dataset.from_tensor_slices(({"title": df_['title'], "text": df_['text']}, df_['fake']))
```

We then apply the function to our training and testing datasets:


```python
dataset = make_dataset(df)
```


```python
test_dataset = make_dataset(test_df)
```

### Train Validation Split, Create Batches

Now we are ready to construct some train & validation split inside our training set. Along with that, we'll also set batches for training set, as training with the original dataset volume might be too inefficient for demonstration purposes.


```python
#shuffle the complete training set
dataset = dataset.shuffle(buffer_size = len(dataset))

#specify training and validation size: 0.7 & 0.2, respectively
train_size = int(0.7*len(dataset))
val_size   = int(0.2*len(dataset))

#pick out first 0.7 as training set, with a batch of 100
train = dataset.take(train_size).batch(100)
#pick out the next 0.2 as validation set, with a batch of 100
val   = dataset.skip(train_size).take(val_size).batch(100)
#test  = dataset.skip(train_size + val_size).batch(100) #last 0.1 of data

#check for the length of the two sets
len(train), len(val)#, len(test)
```




    (158, 45)




```python
test_ds = test_dataset.batch(100)
```

## Create Models

We are now ready to make our models. While our data contains informative columns of both news title and news text, we would like to know whether titles only, text only, or both, would be more effective for detection of fake news. Therefore, we'll be creating three models, with similar strategies, and comparing their accuracies.

### Vectorization layer

No matter which combination we'll be using, we all need to first *vectorize* the input strings. Vectorization refers to processing of the strings into computer readable format, which is, numbers. There are multiple ways of vectorization of strings, such as the [term-document matrix](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16A/blob/master/content/NLP/NLP_1.ipynb), etc. Here, we'll be using the `TextVectorization` function provided by tensorflow. This function would represent each of the words by its rank of frequency in the whole dataset. First, we'll need to define a `standardization` function to supply to the `TextVectorization` function.

Here, the `standardization` function would convert all words to lowercases and remove all punctuations.

```python
def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 
```

Then, with the `TextVectorization` function, we'll make a *vectorize_layer* that transforms each words in the training set, after standardization treatment, into tensors with the words' corresponding *frequency rank*.

```python
from tensorflow.keras import losses
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

import re
import string
```


```python
# only the first 2000 distinct words in the whole set will be tracked
max_tokens = 2000

# only the first 20 words of each headline will be considered
sequence_length = 20

vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=max_tokens, # only consider this many words
    output_mode='int',
    output_sequence_length=sequence_length) 
```
After creating the layers, we would still need to adapt the layer to our set. By adapting it, we will ensure that the vectorization layer have the words' corresponding frequency ranks fixed. Thus whenever we are training or testing with that layer, each distinct word would have that one and only ranking.

```python
vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
```




Then, we'll specify two input layers

```python
from tensorflow import keras
from tensorflow.keras import layers
```


```python
title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```

### Model trains with titles only


```python
x = vectorize_layer(title_input)
x = layers.Embedding(max_tokens, output_dim = 5, name = "embedding_tl")(x)
x = layers.Dropout(0.2)(x)
x = layers.GlobalAveragePooling1D()(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(32, activation='relu')(x)
output = layers.Dense(2, name = "fake")(x)
```


```python
model_tl = keras.Model(
    inputs = title_input,
    outputs = output
)
```


```python
model_tl.summary()
```

    Model: "model"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    title (InputLayer)           [(None, 1)]               0         
    _________________________________________________________________
    text_vectorization (TextVect (None, 20)                0         
    _________________________________________________________________
    embedding_tl (Embedding)     (None, 20, 5)             10000     
    _________________________________________________________________
    dropout (Dropout)            (None, 20, 5)             0         
    _________________________________________________________________
    global_average_pooling1d (Gl (None, 5)                 0         
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 5)                 0         
    _________________________________________________________________
    dense (Dense)                (None, 32)                192       
    _________________________________________________________________
    fake (Dense)                 (None, 2)                 66        
    =================================================================
    Total params: 10,258
    Trainable params: 10,258
    Non-trainable params: 0
    _________________________________________________________________



```python
model_tl.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = model_tl.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = False)
```

    /Users/feishu/opt/anaconda3/envs/PIC16B/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
      [n for n in tensors.keys() if n not in ref_input_names])



```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f84df876c90>




    
![blogpost3_31_1.png](/images/blogpost3_files/blogpost3_31_1.png)
    



```python
model_tl.evaluate(test_ds)
```

    225/225 [==============================] - 0s 1ms/step - loss: 0.0840 - accuracy: 0.9752





    [0.08402662724256516, 0.9751881957054138]



### Model trains with text only


```python
x_ = vectorize_layer(text_input)
x_ = layers.Embedding(max_tokens, output_dim = 5, name = "embedding_tx")(x_)
x_ = layers.Dropout(0.2)(x_)
x_ = layers.GlobalAveragePooling1D()(x_)
x_ = layers.Dropout(0.2)(x_)
x_ = layers.Dense(32, activation='relu')(x_)
output = layers.Dense(2, name = "fake")(x_)
```


```python
model_tx= keras.Model(
    inputs = text_input,
    outputs = output
)
```


```python
model_tx.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
model_tx.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = False)
```

    /Users/feishu/opt/anaconda3/envs/PIC16B/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      [n for n in tensors.keys() if n not in ref_input_names])





    <tensorflow.python.keras.callbacks.History at 0x7f84d88366d0>




```python
model_tx.evaluate(test_ds)
```

    225/225 [==============================] - 1s 6ms/step - loss: 0.1612 - accuracy: 0.9463





    [0.16124901175498962, 0.9462782144546509]



### Model trained with both text and title


```python
both = layers.concatenate([x, x_], axis = 1)
output_both = layers.Dense(2, name = "fake")(both)
```


```python
model_both= keras.Model(
    inputs = [title_input, text_input],
    outputs = output_both
)
```


```python
model_both.summary()
```

    Model: "model_2"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    title (InputLayer)              [(None, 1)]          0                                            
    __________________________________________________________________________________________________
    text (InputLayer)               [(None, 1)]          0                                            
    __________________________________________________________________________________________________
    text_vectorization (TextVectori (None, 20)           0           title[0][0]                      
                                                                     text[0][0]                       
    __________________________________________________________________________________________________
    embedding_tl (Embedding)        (None, 20, 5)        10000       text_vectorization[0][0]         
    __________________________________________________________________________________________________
    embedding_tx (Embedding)        (None, 20, 5)        10000       text_vectorization[1][0]         
    __________________________________________________________________________________________________
    dropout (Dropout)               (None, 20, 5)        0           embedding_tl[0][0]               
    __________________________________________________________________________________________________
    dropout_2 (Dropout)             (None, 20, 5)        0           embedding_tx[0][0]               
    __________________________________________________________________________________________________
    global_average_pooling1d (Globa (None, 5)            0           dropout[0][0]                    
    __________________________________________________________________________________________________
    global_average_pooling1d_1 (Glo (None, 5)            0           dropout_2[0][0]                  
    __________________________________________________________________________________________________
    dropout_1 (Dropout)             (None, 5)            0           global_average_pooling1d[0][0]   
    __________________________________________________________________________________________________
    dropout_3 (Dropout)             (None, 5)            0           global_average_pooling1d_1[0][0] 
    __________________________________________________________________________________________________
    dense (Dense)                   (None, 32)           192         dropout_1[0][0]                  
    __________________________________________________________________________________________________
    dense_1 (Dense)                 (None, 32)           192         dropout_3[0][0]                  
    __________________________________________________________________________________________________
    concatenate (Concatenate)       (None, 64)           0           dense[0][0]                      
                                                                     dense_1[0][0]                    
    __________________________________________________________________________________________________
    fake (Dense)                    (None, 2)            130         concatenate[0][0]                
    ==================================================================================================
    Total params: 20,514
    Trainable params: 20,514
    Non-trainable params: 0
    __________________________________________________________________________________________________



```python
model_both.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
model_both.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = False)
```




    <tensorflow.python.keras.callbacks.History at 0x7f84c388e910>




```python
model_both.evaluate(test_ds)
```

    225/225 [==============================] - 1s 6ms/step - loss: 0.0371 - accuracy: 0.9913





    [0.037129346281290054, 0.991313636302948]



## Embedding Analysis


```python
weights_tl = model_both.get_layer('embedding_tl').get_weights()[0]
weights_tx = model_both.get_layer('embedding_tx').get_weights()[0]# get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()  
```


```python
vocab = vectorize_layer.get_vocabulary()  
```


```python
weights_tl
```




    array([[ 0.10268341,  0.09916885, -0.11284392,  0.12111397, -0.10344727],
           [-0.10890676, -0.13384679,  0.10487777, -0.12676527,  0.13442   ],
           [ 0.05327572,  0.05760193, -0.11840718, -0.00806848, -0.11700375],
           ...,
           [ 0.02861674,  0.02574723,  0.04686182,  0.09227268, -0.03490727],
           [ 0.15639482,  0.09743544, -0.06591856,  0.02370312, -0.01798774],
           [ 0.00066107, -0.01369754, -0.06120993,  0.03191474, -0.09007049]],
          dtype=float32)




```python
from sklearn.decomposition import PCA
```


```python
pca = PCA(n_components=2)
weights_tl = pca.fit_transform(weights_tl)
weights_tx = pca.fit_transform(weights_tx)
```


```python
len(vocab)
```




    2000




```python
embedding_tl = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights_tl[:,0],
    'x1'   : weights_tl[:,1]
})
embedding_tl
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td></td>
      <td>-0.195497</td>
      <td>0.003443</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[UNK]</td>
      <td>0.317627</td>
      <td>0.025609</td>
    </tr>
    <tr>
      <th>2</th>
      <td>said</td>
      <td>-0.105505</td>
      <td>-0.010339</td>
    </tr>
    <tr>
      <th>3</th>
      <td>trump</td>
      <td>0.100054</td>
      <td>-0.045148</td>
    </tr>
    <tr>
      <th>4</th>
      <td>the</td>
      <td>1.743519</td>
      <td>-0.003649</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1995</th>
      <td>tonight</td>
      <td>0.910266</td>
      <td>0.059968</td>
    </tr>
    <tr>
      <th>1996</th>
      <td>repeated</td>
      <td>0.170522</td>
      <td>0.024102</td>
    </tr>
    <tr>
      <th>1997</th>
      <td>projects</td>
      <td>-0.014011</td>
      <td>-0.052086</td>
    </tr>
    <tr>
      <th>1998</th>
      <td>outcome</td>
      <td>-0.117215</td>
      <td>0.046006</td>
    </tr>
    <tr>
      <th>1999</th>
      <td>launch</td>
      <td>-0.029681</td>
      <td>-0.028275</td>
    </tr>
  </tbody>
</table>
<p>2000 rows × 3 columns</p>
</div>




```python
embedding_tx = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights_tx[:,0],
    'x1'   : weights_tx[:,1]
})
```


```python
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
import plotly.express as px 
```


```python
fig = px.scatter( embedding_tl, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_tl))),
                 size_max = 2,
                 hover_name = "word")

fig.show()
```
{% include emb-tx.html %}

```python
from plotly.io import write_html
```


```python
write_html(fig, "emb_tx.html")
```


```python

```
